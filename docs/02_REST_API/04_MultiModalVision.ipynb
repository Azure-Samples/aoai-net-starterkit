{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 REST API | 04 Multi Modal\n",
    "\n",
    "## GPT-4 vision\n",
    "\n",
    "GPT-4 Vision is a multi modal model capable of processing both text and images as input, allowing it to generate insights and information from diverse data sources. This model seamlessly integrates natural language understanding with visual information, enabling a more comprehensive and context-aware understanding of the input data. By handling both text and images, GPT-4 Vision opens up new possibilities for applications that require a holistic analysis of multi modal content.\n",
    "\n",
    "### Further information\n",
    "\n",
    "- [Use GPT-4 Turbo with Vision](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/gpt-with-vision)\n",
    "- [GPT-4 Turbo with Vision concepts](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/gpt-with-vision)\n",
    "\n",
    "## Azure Environment\n",
    "\n",
    "To execute the sample code Azure service specific information like endpoint, api key etc. is needed. Please ensure that you're using gpt-4 and model version `vision` or `vision preview`  ([Details and instructions can be found here](../01_DemoEnvironment/01_Environment.ipynb))\n",
    "\n",
    "We expect the LLM to identify the attraction provided in this image: \n",
    "\n",
    "![LittleMermaid](../../media/img/03_SDK/04_LittleMermaidSmall.jpg) \n",
    "\n",
    "It should deliver the name, location and country of the attraction. \n",
    "\n",
    "The LLM expects a Url to the image which should be analyzed therefore we upload the image to a Storage Account and create a Shared Access Signature which can be used to securely access the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1:  Setup Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "#r \"nuget: DotNetEnv, 2.5.0\"\n",
    "#r \"nuget: System.Text.Json, 7.0.3\"\n",
    "using DotNetEnv;\n",
    "\n",
    "using System.Net;\n",
    "using System.Net.Http;\n",
    "using System.Text.Json.Nodes;\n",
    "using System.Text.Json;\n",
    "using System.IO; \n",
    "\n",
    "static string _configurationFile = @\"../01_DemoEnvironment/conf/application.env\";\n",
    "Env.Load(_configurationFile);\n",
    "\n",
    "string apiBase = Environment.GetEnvironmentVariable(\"SKIT_AOAIVISION_ENDPOINT\") ?? \"SKIT_AOAIVISION_ENDPOINT not found\";\n",
    "string apiKey = Environment.GetEnvironmentVariable(\"SKIT_AOAIVISION_APIKEY\") ?? \"SKIT_AOAIVISION_APIKEY not found\";\n",
    "string deploymentName = Environment.GetEnvironmentVariable(\"SKIT_AOAIVISION_DEPLOYMENTNAME\") ?? \"SKIT_AOAIVISION_DEPLOYMENTNAME not found\";\n",
    "string assetsFolder = Path.Combine(Directory.GetCurrentDirectory(), \"..\", \"..\", \"assets\");\n",
    "\n",
    "string apiVersion = \"2023-12-01-preview\"; //may change in the future"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output\n",
    "```\n",
    "Installed Packages\n",
    "    DotNetEnv, 2.5.0\n",
    "    Newtonsoft.Json, 13.0.1\n",
    "    System.Text.Json, 7.0.3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2:  Retrieve image\n",
    "\n",
    "Images can be provided to an instance of Azure OpenAI in two ways: \n",
    "\n",
    "- through a publicly available URI or \n",
    "- as a base64-encoded string. \n",
    "  \n",
    "The publicly available URI allows the model to access images directly from the web, while the base64-encoded string represents the image's binary data, enabling including images directly in the input payload. \n",
    "\n",
    "This sample provides the image as base64-encoded string. Providing the image using a publicly available (e.g. Azure Storage Account Shared Access Signature) can be done by replacing `data:image/jpeg;base64,...` with the image's URI. \n",
    "\n",
    "```csharp\n",
    "new JsonObject {\n",
    "    { \n",
    "        \"image_url\",  new JsonObject \n",
    "        {\n",
    "            { \"url\", \"<<PUBLIC AVAILABLE URI>>\" }\n",
    "        }\n",
    "    },\n",
    "    {\"type\", \"image_url\" }\n",
    "}\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "string base64Image = Convert.ToBase64String(File.ReadAllBytes(Path.Combine(assetsFolder, \"docs\", \"02_REST_API\", \"LittleMermaid.jpg\")));\n",
    "\n",
    "Console.WriteLine(\"Image converted to base64-encoded string\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```\n",
    "Image converted to base64-encoded string\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "//Define System Prompt\n",
    "string systemMessage = @\" \n",
    "    You are an assistant who helps travel agency staff to find interesting attractions around the world. \n",
    "    You name the place of interest, the city and the country in which the attraction is located. \n",
    "    You do not provide any further information.\n",
    "\"; \n",
    "\n",
    "string userMessage = @\"\n",
    "    Which place of interest is this?\n",
    "\";\n",
    "\n",
    "JsonObject requestPayload = new JsonObject\n",
    "{\n",
    "    { \"messages\", new JsonArray\n",
    "        {\n",
    "            new JsonObject\n",
    "            {\n",
    "                { \"content\", systemMessage },\n",
    "                { \"role\", \"system\" }\n",
    "            },\n",
    "            new JsonObject \n",
    "            {\n",
    "                { \"content\", \n",
    "                    new JsonArray {\n",
    "                        new JsonObject {\n",
    "                            { \"text\", userMessage },\n",
    "                            { \"type\", \"text\" }\n",
    "                        }, \n",
    "                        new JsonObject {\n",
    "                            { \n",
    "                                \"image_url\",  new JsonObject \n",
    "                                {\n",
    "                                    { \"url\", string.Concat(\"data:image/jpeg;base64,\", base64Image) }\n",
    "                                }\n",
    "                            },\n",
    "                            {\"type\", \"image_url\" }\n",
    "                        }\n",
    "                    }\n",
    "                }, \n",
    "                { \"role\", \"user\" }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    { \"max_tokens\", 200 },\n",
    "    { \"temperature\", 0.7 },\n",
    "    { \"frequency_penalty\", 0 },\n",
    "    { \"presence_penalty\", 0 },\n",
    "    { \"top_p\", 0.95 },\n",
    "    { \"model\", \"gpt4vision\"}\n",
    "};\n",
    "\n",
    "string payload = JsonSerializer.Serialize(requestPayload, new JsonSerializerOptions\n",
    "{\n",
    "    WriteIndented = true // Optional: to make the JSON string more readable\n",
    "});\n",
    "\n",
    "Console.WriteLine(\"Request payload created...\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```\n",
    "Request payload created...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Call OpenAI Endpoint\n",
    "\n",
    "The code cell is using an instance of `HttpClient` to call the REST API of the deployed Azure OpenAI instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "string endpoint = $\"{apiBase}openai/deployments/{deploymentName}/chat/completions?api-version={apiVersion}\";\n",
    "\n",
    "using (HttpClient httpClient = new HttpClient())\n",
    "{\n",
    "    httpClient.BaseAddress = new Uri(endpoint);\n",
    "    httpClient.DefaultRequestHeaders.Add(\"api-key\",apiKey);\n",
    "    httpClient.DefaultRequestHeaders.Accept.Add(new System.Net.Http.Headers.MediaTypeWithQualityHeaderValue(\"application/json\"));\n",
    "\n",
    "    var stringContent = new StringContent(payload, Encoding.UTF8, \"application/json\");\n",
    "\n",
    "    var response = await httpClient.PostAsync(endpoint, stringContent);\n",
    "\n",
    "    if (response.IsSuccessStatusCode)\n",
    "    {\n",
    "        using (var responseStream = await response.Content.ReadAsStreamAsync())\n",
    "        {\n",
    "            // Parse the JSON response using JsonDocument\n",
    "            using (var jsonDoc = await JsonDocument.ParseAsync(responseStream))\n",
    "            {\n",
    "                // Access the message content dynamically\n",
    "                JsonElement jsonElement = jsonDoc.RootElement;\n",
    "                string messageContent = jsonElement.GetProperty(\"choices\")[0].GetProperty(\"message\").GetProperty(\"content\").GetString();\n",
    "\n",
    "                // Output the message content\n",
    "                Console.WriteLine(\"Output: \" + messageContent);\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    else\n",
    "    {\n",
    "        Console.WriteLine($\"Error: {response}\");\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```\n",
    "Output: The Little Mermaid statue, Copenhagen, Denmark.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that you understand how to use the REST API to interact with Azure OpenAI, explore how to use the Azure OpenAI client library for .NET in the [next notebook](../03_SDK/01_ChatCompletion.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".NET (C#)",
   "language": "C#",
   "name": ".net-csharp"
  },
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4,
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
