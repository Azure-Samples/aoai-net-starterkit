{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 SDK | 02 ChatCompletion Streaming\n",
    "\n",
    "## Why Streaming?\n",
    "\n",
    "In our [previous notebook](./01_ChatCompletion.ipynb) we covered how chat completion work. Sending _System_, _Assistance_ and _User_ messages as prompt, and receiving in a single payload the response. This is great for a lot of use cases, but it has some limitations:\n",
    "\n",
    "- In case the response is too long, it might take longer to receive the response.\n",
    "- If the calculation of the response takes too long, the user might think the LLM model is not responding.\n",
    "\n",
    "For these (and more) scenarios we can use the streaming option. With this option, we can call the LLM with _Async_ mode, and receive the response in chunks. This way we can start receiving the response as soon as the LLM starts calculating it, and we can also receive the response in chunks, so we can show the user the response as soon as we receive it.\n",
    "\n",
    "### Further reasons to use streaming:\n",
    "\n",
    "__Real-time Interactions:__ Some applications require real-time interactions with the API, where the user's input is processed on-the-fly. Streaming allows for more immediate feedback, making it suitable for applications such as AI agents, real-time translations, or interactive tutorials.\n",
    "\n",
    "__Handling Large Data:__ If an application needs to send or receive large amounts of data, breaking it into smaller chunks and streaming it can be more efficient than sending it all at once. This can help avoid timeouts or other issues associated with large data transfers.\n",
    "\n",
    "__Adaptive Responses:__ In some applications, the response from the model might influence subsequent inputs. For example, if a chatbot gives a particular answer, the user might ask a follow-up question. Streaming allows this kind of adaptive interaction.\n",
    "\n",
    "\n",
    "__Improved User Experience:__ For end-users, streaming can provide a smoother, more interactive experience, especially in applications where rapid back-and-forth communication is necessary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure Environment\n",
    "\n",
    "You can create the necessary instance of Azure OpenAI and deploy an embedding model by executing one of the deployment options [mentioned here](../../create_env/src/AzureCLI/CreateEnv.azcli). To execute the sample code service specific information is needed ([Details and instructions here](../01_DemoEnvironment/01_Environment.ipynb)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create OpenAIClient\n",
    "\n",
    "The OpenAIClient from Azure.AI.OpenAI is a .NET client library that acts as the centralized point for all .NET functionality that want to interact with a deployed Azure OpenAI Large Language Model. It provides methods to access the OpenAI REST APIs for various tasks such as text completion, text embedding, and chat completion, etc.. It also allows developers to specify the model, engine, and options for each request, such as temperature, frequency penalty, presence penalty, and stop sequences. \n",
    "\n",
    "The OpenAIClient can connect to any Azure OpenAI resource or to the non-Azure OpenAI inference endpoint, making it a versatile and powerful tool for .NET development with OpenAI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "#r \"nuget: Azure.AI.OpenAI, 1.0.0-beta.12\"\n",
    "#r \"nuget: DotNetEnv, 2.5.0\"\n",
    "\n",
    "using Azure; \n",
    "using Azure.AI.OpenAI;\n",
    "using DotNetEnv;\n",
    "using System.IO;\n",
    "using System.Text.Json; \n",
    "\n",
    "//configuration file is created during environment creation\n",
    "//if you skipped the deployment just remove the code and provide values from your deployment\n",
    "static string _configurationFile = @\"../01_DemoEnvironment/conf/application.env\";\n",
    "Env.Load(_configurationFile);\n",
    "\n",
    "string oAiApiKey = Environment.GetEnvironmentVariable(\"SKIT_AOAI_APIKEY\") ?? \"SKIT_AOAI_APIKEY not found\";\n",
    "string oAiEndpoint = Environment.GetEnvironmentVariable(\"SKIT_AOAI_ENDPOINT\") ?? \"SKIT_AOAI_ENDPOINT not found\";\n",
    "string chatCompletionDeploymentName = Environment.GetEnvironmentVariable(\"SKIT_CHATCOMPLETION_DEPLOYMENTNAME\") ?? \"SKIT_CHATCOMPLETION_DEPLOYMENTNAME not found\";\n",
    "\n",
    "string assetsFolder = Path.Combine(Directory.GetCurrentDirectory(), \"..\", \"..\", \"assets\");\n",
    "\n",
    "AzureKeyCredential azureKeyCredential = new AzureKeyCredential(oAiApiKey);\n",
    "OpenAIClient openAIClient = new OpenAIClient(new Uri(oAiEndpoint), azureKeyCredential);\n",
    "\n",
    "Console.WriteLine($\"OpenAI Client created...\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```\n",
    "Installed Packages\n",
    "    Azure.AI.OpenAI, 1.0.0-beta.12\n",
    "    DotNetEnv, 2.5.0\n",
    "\n",
    "OpenAI Client created...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Compose ChatCompletionsOptions\n",
    "\n",
    "Each chat would follow similar structure, where _System_, _Agent_ and _User_ messages are added in sequence. Parameters, such as _Temperature_ could be set per call.\n",
    "The model would then response with the most likely next message. In the __Streaming__ option we would be able to process the response in chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "//Define System Prompt\n",
    "string system = @\" \n",
    "    You summarize technical information provided to you. \n",
    "    The summarization is focused on key elements and messages from the text. \n",
    "    You response with one paragraph and in a language which is easy understandable for an experienced .NET developer.\n",
    "\";\n",
    "\n",
    "//Compose Chat (Simplified - No few shot learning in this example)\n",
    "ChatCompletionsOptions chatCompletionsOptions = new ChatCompletionsOptions();\n",
    "\n",
    "chatCompletionsOptions.Messages.Add(new ChatRequestSystemMessage(system));\n",
    "chatCompletionsOptions.Messages.Add(new ChatRequestUserMessage(File.ReadAllText(Path.Combine(assetsFolder, \"docs\", \"03_SDK\", \"aci_documentation.txt\"))));\n",
    "\n",
    "//Request Properties\n",
    "chatCompletionsOptions.MaxTokens = 500;\n",
    "chatCompletionsOptions.Temperature = 0.0f;\n",
    "chatCompletionsOptions.NucleusSamplingFactor = 0.0f;\n",
    "chatCompletionsOptions.FrequencyPenalty = 0.7f;\n",
    "chatCompletionsOptions.PresencePenalty = 0.7f;\n",
    "chatCompletionsOptions.StopSequences.Add(\"\\n\"); \n",
    "chatCompletionsOptions.DeploymentName = chatCompletionDeploymentName;\n",
    "//Choices per prompt\n",
    "chatCompletionsOptions.ChoiceCount = 1;\n",
    "\n",
    "Console.WriteLine(\"ChatCompletionsOptions created...\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```\n",
    "ChatCompletionsOptions created...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Call Streaming API\n",
    "\n",
    "The API will response in chunks of tokens. One can display them as outlined in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "StreamingResponse<StreamingChatCompletionsUpdate> response = await openAIClient.GetChatCompletionsStreamingAsync(chatCompletionsOptions);\n",
    "\n",
    "string responseContent = string.Empty;\n",
    "await foreach (var messages in response.EnumerateValues())\n",
    "{\n",
    "    if(messages.ContentUpdate != null)\n",
    "    {\n",
    "        Console.WriteLine($\"Arrived chunk: {messages.ContentUpdate}\");\n",
    "        responseContent += messages.ContentUpdate;        \n",
    "    }\n",
    "}\n",
    "\n",
    "Console.WriteLine($\"Response: {responseContent.Substring(0, 50)}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.1: Call Streaming API - typing \n",
    "\n",
    "The following is show casing the typing effect. The response is received in chunks and written to the console, adding line breaks between specific word count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "StreamingResponse<StreamingChatCompletionsUpdate> response = await openAIClient.GetChatCompletionsStreamingAsync(chatCompletionsOptions);\n",
    "\n",
    "string responseContent = string.Empty;\n",
    "int wordsinline = 15;\n",
    "\n",
    "int pos = 0;\n",
    "await foreach (var messages in response.EnumerateValues())\n",
    "{\n",
    "    if(messages.ContentUpdate != null)\n",
    "    {\n",
    "        pos = (pos == wordsinline) ? 0 : pos + 1;\n",
    "        if (pos == 0) Console.WriteLine();\n",
    "        Console.Write($\"{messages.ContentUpdate}\");\n",
    "        responseContent += messages.ContentUpdate;        \n",
    "    }\n",
    "}\n",
    "\n",
    "Console.WriteLine(\"\");\n",
    "// Once streaming is complete, the response object contains the completion\n",
    "Console.WriteLine($\"Response: {responseContent.Substring(0, 50)}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.2: Cancellation token\n",
    "\n",
    "In few scenarios calls might be canceled before the entire response is received. This is show cased in this example.\n",
    "In this example we are using the DeepDev package, lets install it.\n",
    "\n",
    "The following example demonstrate a cancelled call. Using a time driven cancellation token, the call will be cancelled after the specified milliseconds. Using a tokenizer to convert the textual responses to tokens to ensure cost is calculated correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "#r \"nuget: Microsoft.DeepDev.TokenizerLib\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```\n",
    "Installed Packages\n",
    "    Microsoft.DeepDev.TokenizerLib, 1.3.2\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "using System;\n",
    "using System.Threading;\n",
    "using Microsoft.DeepDev;\n",
    "var IM_START = \"<|im_start|>\";\n",
    "var IM_END = \"<|im_end|>\";\n",
    "\n",
    "var SpecialTokens = new Dictionary<string, int>{\n",
    "                                            { IM_START, 100264},\n",
    "                                            { IM_END, 100265},\n",
    "                                        };\n",
    "\n",
    "var tokenizer = await TokenizerBuilder.CreateByModelNameAsync(\"gpt-3.5-turbo\",SpecialTokens);\n",
    "\n",
    "var cts = new CancellationTokenSource(TimeSpan.FromMilliseconds(920)); // Cancel after xxx mili-seconds\n",
    "string responseContent = string.Empty;\n",
    "\n",
    "// try {\n",
    "    StreamingResponse<StreamingChatCompletionsUpdate> response  = await openAIClient.GetChatCompletionsStreamingAsync(chatCompletionsOptions, cancellationToken: cts.Token);\n",
    "    int wordsinline = 15;\n",
    "\n",
    "    int pos = 0;\n",
    "    await foreach (var messages in response.EnumerateValues())\n",
    "    {\n",
    "        if(messages.ContentUpdate != null)\n",
    "        {\n",
    "            pos = (pos == wordsinline) ? 0 : pos + 1;\n",
    "            if (pos == 0) Console.WriteLine();\n",
    "            Console.Write($\"{messages.ContentUpdate}\");\n",
    "            responseContent += messages.ContentUpdate;        \n",
    "        }\n",
    "    }    \n",
    "// }catch (Exception ex) {\n",
    "//     Console.WriteLine($\"An error occurred: {ex.Message}\");\n",
    "//     Console.WriteLine(\"Operation was canceled\");\n",
    "//     Console.WriteLine($\"Response: {responseContent}\");\n",
    "//     Console.Out.Flush();\n",
    "//     // var encoded = tokenizer.Encode(responseContent, new HashSet<string>(SpecialTokens.Keys));\n",
    "\n",
    "//     // Console.WriteLine($\"Tokens used up to cancellation: {responseContent.Length} , tokens: {encoded.Count}\");\n",
    "// }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- The concept of Embeddings allows transforming information into a numerical representation preserving the semantic context of the information: [Demo Embeddings](../04_Embeddings/01_BasicEmbeddings.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".NET (C#)",
   "language": "C#",
   "name": ".net-csharp"
  },
  "language_info": {
   "name": "polyglot-notebook"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "languageName": "csharp",
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
