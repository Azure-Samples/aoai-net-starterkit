{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 SDK | 04 Multi Modal Vision\n",
    "\n",
    "## GPT-4 vision\n",
    "\n",
    "GPT-4 Vision is a multi modal model capable of processing both text and images as input, allowing it to generate insights and information from diverse data sources. This model seamlessly integrates natural language understanding with visual information, enabling a more comprehensive and context-aware understanding of the input data. By handling both text and images, GPT-4 Vision opens up new possibilities for applications that require a holistic analysis of multi modal content.\n",
    "\n",
    "### Further information\n",
    "\n",
    "- [Use GPT-4 Turbo with Vision](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/gpt-with-vision)\n",
    "- [GPT-4 Turbo with Vision concepts](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/gpt-with-vision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure Environment\n",
    "\n",
    "To execute the sample code Azure service specific information like endpoint, api key etc. is needed. Please ensure that you're using gpt-4 and model version `vision` or `vision preview`  ([Details and instructions can be found here](../01_DemoEnvironment/01_Environment.ipynb))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create OpenAIClient\n",
    "\n",
    "The OpenAIClient from Azure.AI.OpenAI is a .NET client library that acts as the centralized point for all .NET functionality that want to interact with a deployed Azure OpenAI Large Language Model. It provides methods to access the OpenAI REST APIs for various tasks such as text completion, text embedding, and chat completion, etc.. It also allows developers to specify the model, engine, and options for each request, such as temperature, frequency penalty, presence penalty, and stop sequences. \n",
    "\n",
    "The OpenAIClient can connect to any Azure OpenAI resource or to the non-Azure OpenAI inference endpoint, making it a versatile and powerful tool for .NET development with OpenAI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "#r \"nuget: Azure.AI.OpenAI, 1.0.0-beta.12\"\n",
    "#r \"nuget: DotNetEnv, 2.5.0\"\n",
    "\n",
    "using Azure; \n",
    "using Azure.AI.OpenAI;\n",
    "using DotNetEnv;\n",
    "using System.IO;\n",
    "using System.Text.Json; \n",
    "\n",
    "//configuration file is created during environment creation\n",
    "//if you skipped the deployment just remove the code and provide values from your deployment\n",
    "static string _configurationFile = @\"../01_DemoEnvironment/conf/application.env\";\n",
    "Env.Load(_configurationFile);\n",
    "\n",
    "string oAiApiKey = Environment.GetEnvironmentVariable(\"SKIT_AOAIVision_APIKEY\") ?? \"SKIT_AOAIVISION_APIKEY not found\";\n",
    "string oAiEndpoint = Environment.GetEnvironmentVariable(\"SKIT_AOAIVision_ENDPOINT\") ?? \"SKIT_AOAIVISION_ENDPOINT not found\";\n",
    "string chatCompletionDeploymentName = Environment.GetEnvironmentVariable(\"SKIT_AOAIVISION_DEPLOYMENTNAME\") ?? \"SKIT_AOAIVISION_DEPLOYMENTNAME not found\";\n",
    "string storageConnectionString = Environment.GetEnvironmentVariable(\"SKIT_STORAGE_CONNECTIONSTRING\") ?? \"SKIT_STORAGE_CONNECTIONSTRING not found\";\n",
    "string assetsFolder = Path.Combine(Directory.GetCurrentDirectory(), \"..\", \"..\", \"assets\");\n",
    "\n",
    "AzureKeyCredential azureKeyCredential = new AzureKeyCredential(oAiApiKey);\n",
    "OpenAIClient openAIClient = new OpenAIClient(new Uri(oAiEndpoint), azureKeyCredential);\n",
    "\n",
    "\n",
    "Console.WriteLine($\"OpenAI Client created...\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```\n",
    "Installed Packages\n",
    "    Azure.AI.OpenAI, 1.0.0-beta.12\n",
    "    DotNetEnv, 2.5.0\n",
    "\n",
    "OpenAI Client created...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Upload image to Azure Storage and crate Shared Access Signature\n",
    "\n",
    "We expect the LLM to identify the attraction provided in this image: \n",
    "\n",
    "![LittleMermaid](../../media/img/03_SDK/04_LittleMermaidSmall.jpg) \n",
    "\n",
    "It should deliver the name, location and country of the attraction. \n",
    "\n",
    "Images can be provided to an instance of Azure OpenAI in two ways: \n",
    "\n",
    "- through a publicly available URI or \n",
    "- as a base64-encoded string. \n",
    "  \n",
    "The current version of the c# OpenAI SDK (1.0.0-beta.12) supports URI provided images. A sample notebook how to provide images as base64-encoded string can be found [here (04_MultiModalVision.ipynb)](../02_REST_API/04_MultiModalVision.ipynb).\n",
    "\n",
    "In this sample we upload the image to a Storage Account and create a Shared Access Signature which can be used to securely provide the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "#r \"nuget: Azure.Storage.Blobs\"\n",
    "\n",
    "using Azure.Storage.Blobs;\n",
    "using Azure.Storage.Sas;\n",
    "\n",
    "string containerName = \"skit\";\n",
    "string blobName = \"LittleMermaid.jpg\";\n",
    "string localFilePath = \"../../assets/docs/03_SDK/LittleMermaid.jpg\";\n",
    "string sasUrl = \"\";\n",
    "\n",
    "BlobServiceClient blobServiceClient = new BlobServiceClient(storageConnectionString);\n",
    "BlobContainerClient containerClient = blobServiceClient.GetBlobContainerClient(containerName); \n",
    "containerClient.CreateIfNotExists();\n",
    "\n",
    "BlobClient blobClient = containerClient.GetBlobClient(blobName);\n",
    "using (FileStream fileStream = new FileStream(localFilePath, FileMode.Open)){\n",
    "    await blobClient.UploadAsync(fileStream, true);\n",
    "    BlobSasBuilder sasBuilder = new BlobSasBuilder() {\n",
    "        BlobContainerName = containerName,\n",
    "        BlobName = blobClient.Name,\n",
    "        Resource = \"b\"\n",
    "    };\n",
    "    sasBuilder.ExpiresOn = DateTimeOffset.UtcNow.AddDays(1);\n",
    "    sasBuilder.SetPermissions(BlobContainerSasPermissions.Read);\n",
    "\n",
    "    sasUrl = blobClient.GenerateSasUri(sasBuilder).ToString();\n",
    "}\n",
    "Console.WriteLine($\"Blob uploaded to {sasUrl}\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output: \n",
    "```\n",
    "Blob uploaded to <<Url with SaS>>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Compose ChatCompletionsOptions\n",
    "\n",
    "Each chat would follow similar structure, where _System_, _Agent_ and _User_ messages are added in sequence. Parameters, such as _Temperature_ could be set per call. We can provide the the image url and the user message containing instructions how to process the image using: \n",
    "\n",
    "```csharp\n",
    "new ChatRequestUserMessage(\n",
    "    new ChatMessageTextContentItem(userMessage),\n",
    "    new ChatMessageImageContentItem(new Uri(sasUrl))\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "//Define System Prompt\n",
    "string systemMessage = @\" \n",
    "    You are an assistant who helps travel agency staff to find interesting attractions around the world. \n",
    "    You name the place of interest, the city and the country in which the attraction is located. \n",
    "    You do not provide any further information.\n",
    "\"; \n",
    "\n",
    "string userMessage = @\"\n",
    "    Which place of interest is this?\n",
    "\";\n",
    "\n",
    "//Compose Chat (Simplified - No few shot learning in this example)\n",
    "ChatCompletionsOptions chatCompletionsOptions = new ChatCompletionsOptions()\n",
    "{\n",
    "    DeploymentName = chatCompletionDeploymentName,\n",
    "    Messages =\n",
    "    {\n",
    "        new ChatRequestSystemMessage(systemMessage),\n",
    "        new ChatRequestUserMessage(userMessage),\n",
    "        new ChatRequestUserMessage(\n",
    "            new ChatMessageTextContentItem(userMessage),\n",
    "            new ChatMessageImageContentItem(new Uri(sasUrl))\n",
    "        )\n",
    "    },\n",
    "    MaxTokens = 100, \n",
    "    NucleusSamplingFactor = 0.1f,\n",
    "    Temperature = 0.1f,\n",
    "};\n",
    "\n",
    "Console.WriteLine($\"ChatCompletionsOptions created...\");\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```\n",
    "ChatCompletionsOptions created...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Call ChatCompletion API\n",
    "\n",
    "The final step is to call the `GetChatCompletionAsync()` method to get the response from the LLM.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "Response<ChatCompletions> response = await openAIClient.GetChatCompletionsAsync(chatCompletionsOptions);\n",
    "Console.WriteLine(response.Value.Choices[0].Message.Content);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```\n",
    "The Little Mermaid statue, Copenhagen, Denmark.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- The concept of Embeddings allows transforming information into a numerical representation preserving the semantic context of the information: [Demo Embeddings](../04_Embeddings/01_BasicEmbeddings.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".NET (C#)",
   "language": "C#",
   "name": ".net-csharp"
  },
  "language_info": {
   "name": "polyglot-notebook"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "languageName": "csharp",
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
